{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monocular Visual Odometry\n",
    "\n",
    "Akshay Aggarwal\n",
    "\n",
    "This is a simple implementation of monocular visual odometry using optical flow. the dataset used here is provided by [KITTI](http://www.cvlibs.net/datasets/kitti/eval_odometry.php)\n",
    "For simplicity ( and efficieny), I have used the grayscale dataset. The data set also comes with the calibration files and ground truth pose. \n",
    "\n",
    "\n",
    "## Citation\n",
    "\n",
    "@INPROCEEDINGS{Geiger2012CVPR,\n",
    "author = {Andreas Geiger and Philip Lenz and Raquel Urtasun},\n",
    "title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},\n",
    "booktitle = {Conference on Computer Vision and Pattern\tRecognition (CVPR)},\n",
    "year = {2012}\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://github.com/uoip/monoVO-python\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Parameters for lucas kanade optical flow\n",
    "winSize  = (15,15)\n",
    "maxLevel = 2\n",
    "criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    "0, 0.001\n",
    "\n",
    "def featureTracking(prev_img, next_img, prev_pts, winSize, maxLevel, criteria):\n",
    "    term_crit = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_COUNT, 30, 0.1)\n",
    "    \n",
    "    # calculate optical flow\n",
    "    next_pts, status, error = cv2.calcOpticalFlowPyrLK(prev_img, next_img, prev_pts, winSize, maxLevel, criteria, 0, 0.001)\n",
    "    \n",
    "    #TODO: get rid of poins which are outside the frame\n",
    "    \n",
    "    return(next_pts, status, error)\n",
    "\n",
    "\n",
    "#uses FAST as of now, modify parameters as necessary\n",
    "def featureDetection_fast(img,fast_threshold = 20,nonmaxSuppression = True  ):\n",
    "    # Initiate FAST object \n",
    "    fast = cv2.FastFeatureDetector_create(fast_threshold, nonmaxSuppression)\n",
    "    # find the keypoints\n",
    "    points1 = fast.detect(img,)\n",
    "    return pts\n",
    "\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 1000,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "def featureDetection_ShiTomasi(img, feature_params):\n",
    "    pts = cv2.goodFeaturesToTrack(img, mask = None, **feature_params)\n",
    "    return pts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Loading the dataset\n",
    "Tools courtesy Lee Clement and his group at University of Toronto. \n",
    "[pykitti](https://github.com/utiasSTARS/pykitti)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import datetime as dt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "frame_range = range(0,200)\n",
    "base_dir = './KITTI_excerpt1'\n",
    "sequencepath = basedir+'/sequences/00/'\n",
    "posepath = basedir+'/poses/'\n",
    "\n",
    "\n",
    "class load_dataset(object):\n",
    "    def __init__(self, base_dir, frame_range = None):\n",
    "        self.base_dir = base_dir\n",
    "        self.sequence_path = self.base_dir +'/sequences/00/'\n",
    "        self.pose_path = self.base_dir +'/poses/'\n",
    "        self.calib_path = self.sequence_path+'calib.txt'\n",
    "        self.frame_range = frame_range\n",
    "\n",
    "    def read_calib_file(self, filepath):\n",
    "        \"\"\"Read in a calibration file and parse into a dictionary.\"\"\"\n",
    "        self.data = {}\n",
    "\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                key, value = line.split(':', 1)\n",
    "                # The only non-float values in these files are dates, which\n",
    "                # we don't care about anyway\n",
    "                try:\n",
    "                    self.data[key] = np.array([float(x) for x in value.split()])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    def load_calib(self):\n",
    "        \"\"\"Load and compute intrinsic and extrinsic calibration parameters.\"\"\"\n",
    "        # We'll build the calibration parameters as a dictionary, then\n",
    "        # convert it to a namedtuple to prevent it from being modified later\n",
    "        data = {}\n",
    "        \n",
    "        # Load the calibration file\n",
    "        self.calib_filedata = read_calib_file(self.calib_path)\n",
    "\n",
    "        # Create 3x4 projection matrices\n",
    "        P_rect_00 = np.reshape(filedata['P0'], (3, 4))\n",
    "        P_rect_10 = np.reshape(filedata['P1'], (3, 4))\n",
    "        P_rect_20 = np.reshape(filedata['P2'], (3, 4))\n",
    "        P_rect_30 = np.reshape(filedata['P3'], (3, 4))\n",
    "\n",
    "        # Compute the camera intrinsics\n",
    "        data['K_cam0'] = P_rect_00[0:3, 0:3]\n",
    "        data['K_cam1'] = P_rect_10[0:3, 0:3]\n",
    "        data['K_cam2'] = P_rect_20[0:3, 0:3]\n",
    "        data['K_cam3'] = P_rect_30[0:3, 0:3]\n",
    "\n",
    "        self.calib = namedtuple('CalibData', data.keys())(*data.values())\n",
    "        self.focal = calib[0][0][0] # Camera focal length\n",
    "        self.pp = [calib[0][0][2],calib[0][1][2]] #camera pricipal point\n",
    "        \n",
    "    def load_timestamps(self):\n",
    "        \"\"\"Load timestamps from file.\"\"\"\n",
    "        timestamp_file = self.sequence_path+'times.txt'\n",
    "\n",
    "        # Read and parse the timestamps\n",
    "        timestamps = []\n",
    "        with open(timestamp_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                t = dt.timedelta(seconds=float(line))\n",
    "                timestamps.append(t)\n",
    "\n",
    "        # Subselect the chosen range of frames, if any\n",
    "        self.timestamps = [timestamps[i] for i in frame_range]\n",
    "\n",
    "        # print('Found ' + str(len(timestamps)) + ' timestamps...')\n",
    "        # print('done.')\n",
    "    \n",
    "    def load_poses(self):\n",
    "        \"\"\"Load ground truth poses from file.\"\"\"\n",
    "        # Read and parse the poses\n",
    "        try:\n",
    "            self.pose = []\n",
    "            with open(self.pose_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    T = line.strip().split()\n",
    "                    xyz = [float(T[3]), float(T[7]), float(T[11])]\n",
    "                    self.pose.append(xyz)\n",
    "            #print('done.')\n",
    "        except FileNotFoundError:\n",
    "            print('Ground truth poses are not avaialble for sequence')\n",
    "    \n",
    "    def load_gray(self):\n",
    "        im_path = os.path.join(self.sequence_path, 'image_0', '*.png')\n",
    "        im_files = sorted(glob.glob(imL_path))\n",
    "\n",
    "        # Subselect the chosen range of frames, if any\n",
    "        if self.frame_range:\n",
    "            im_files = [im_files[i] for i in self.frame_range]\n",
    "\n",
    "        for imfile in im_files:\n",
    "            im = mpimg.imread(imfile)\n",
    "            self.gray.append(im)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class movo(object):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.859327295134\n"
     ]
    }
   ],
   "source": [
    "# Get Absolute Scale between ground truth poses of frame_id and frame_id-1\n",
    "\n",
    "def getAbsoluteScale(pose, frame_id):  \n",
    "\n",
    "    prev = pose[frame_id-1]\n",
    "    curr = pose[frame_id]\n",
    "    scale = np.sqrt(sum((c-p)**2 for c,p in zip(curr,prev)))\n",
    "    return(scale)\n",
    "\n",
    "scale = getAbsoluteScale(pose,2)\n",
    "print(scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  process first two frames\n",
    "\n",
    "def initProcess(img1, img2):\n",
    "    \n",
    "    focal = calib[0][0][0] # Camera focal length\n",
    "    pp = [calib[0][0][2],calib[0][1][2]] #camera pricipal point\n",
    "    \n",
    "    # Process 1st frame\n",
    "    pts1 = featureDetection_fast(img1)\n",
    "    \n",
    "    # Process 2nd frame\n",
    "    \n",
    "    pts2, status, error = featureTracking(img1, img2, pts1, winSize, maxLevel, criteria)\n",
    "    E, mask = cv2.findEssentialMat(pts2, pts1, focal, pp, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, Rvec, Tvec, mask = cv2.recoverPose(E, pts2, pts1, focal, pp)\n",
    "    return(img2, pts1, Rvec, Tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prev_img, prev_pts, curr_Rvec, curr_Tvec = initProcess(img1, img2)\n",
    "\n",
    "def Process(prev_img, curr_img, prev_pts, frame_id):\n",
    "    \n",
    "    curr_pts, status, error = featureTracking(prev_img, curr_img, prev_pts, winSize, maxLevel, criteria)\n",
    "    \n",
    "    E, mask = cv2.findEssentialMat(curr_pts, prev_pts, focal, pp, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, Rvec, Tvec, mask = cv2.recoverPose(E, curr_pts, prev_pts, focal, pp)\n",
    "    \n",
    "    absoluteScale = getAbsoluteScale(pose,frame_id)\n",
    "    if absoluteScale>0.1:\n",
    "        curr_Tvec += absoluteScale*curr_Rvec.dot(Tvec)\n",
    "        curr_Rvec = \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:car]",
   "language": "python",
   "name": "conda-env-car-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
