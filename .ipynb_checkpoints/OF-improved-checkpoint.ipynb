{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python Standard libraries\n",
    "import time\n",
    "from collections import deque\n",
    "import copy\n",
    "import warnings\n",
    "import csv\n",
    "# Image processing and numerical calculations\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Generic set up and selection of detector, descriptor and matcher objects\n",
    "DETECTOR_TYPES = {1: \"FAST\", 2: \"GridFAST\", 3: \"BRISK\"}\n",
    "DESCRIPTOR_TYPES = {1: \"FREAK\", 2: \"BRISK\"}\n",
    "MATCHER_TYPES = {1: \"BRUTEFORCE\"}\n",
    "DETECTOR = DETECTOR_TYPES[2]\n",
    "DESCRIPTOR = DESCRIPTOR_TYPES[1]\n",
    "MATCHER = MATCHER_TYPES[1]\n",
    "MTCH_METHODS = [cv2.NORM_L1, cv2.NORM_L2, cv2.NORM_HAMMING]\n",
    "MTCH_METHOD = MTCH_METHODS[2] # HAMMING for binary descriptors\n",
    "MATCH_QTY = 1500 # number keypoints desired per image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for Lucas-Kanade optical flow\n",
    "LK_PARAMS = dict(winSize=(10, 10),\n",
    "maxLevel=2,\n",
    "criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,10, 0.03))\n",
    "# This parameter adjusts resilience of backwards calculated points from LK\n",
    "# optical flow. The higher the number, the more a point will persist from\n",
    "# lucas kanade. Basically, this means points stick around on objects with\n",
    "# large motions or have a higher chance to reattach to different pixels.\n",
    "# The downside is the higher this number is, the more likely flow vectors\n",
    "# will be incorrect under large motions. Better to keep it low (20 max) and\n",
    "# introduce new points rather than retain old, often incorrect ones.\n",
    "LK_BACKCALC_PIXEL_DEVIATION = 25\n",
    "\n",
    "# Number of (x,y) points retained for keypoint track history\n",
    "MAX_TRACK_LENGTH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Image(object):\n",
    "    \"\"\"\n",
    "    Base container class used in the image processing library\n",
    "    \"\"\"\n",
    "    def __init__(self, frame, frame_time=None):\n",
    "        height = frame.shape[0]\n",
    "        width = frame.shape[1]\n",
    "        size = (width, height)\n",
    "        self.size = size\n",
    "        if frame_time is None:\n",
    "            self.frame_time = time.time()\n",
    "        else:\n",
    "            self.frame_time = frame_time\n",
    "            self.clr_img = frame\n",
    "            self.gry_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Lazily set attributes. Test for existence before use\n",
    "        self.kp = []\n",
    "        self.desc = []\n",
    "        self.tracks = []\n",
    "    \n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "    \n",
    "    def get_color(self):\n",
    "        return self.clr_img\n",
    "    \n",
    "    def get_grey(self):\n",
    "        return self.gry_img\n",
    "    \n",
    "    def get_time(self):\n",
    "        return self.frame_time\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.size\n",
    "\n",
    "    def set_details(self, kp, desc):\n",
    "        self.kp = kp\n",
    "        self.desc = desc\n",
    "\n",
    "    def set_flow_tracks(self, tracks):\n",
    "        self.tracks = tracks\n",
    "\n",
    "    def get_kp(self):\n",
    "        return self.kp\n",
    "\n",
    "    def get_desc(self):\n",
    "        return self.desc\n",
    "\n",
    "    def get_flow_tracks(self):\n",
    "        return self.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageCompare(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Enacts a number of comparison techniques to track key points between\n",
    "        frames from a video source or individual pictures.\n",
    "        Automatically tracks time and retains a history of previous image data.\n",
    "        Class loads ImageData instances with single instance of OpenCV\n",
    "        descriptor and detector classes.\n",
    "        \"\"\"\n",
    "        # Data attributes storing frame data\n",
    "        self.frame_prev = None\n",
    "        self.frame_curr = None\n",
    "        # Data attributes storing comparison data\n",
    "        self.matches_curr = None\n",
    "\n",
    "        # creates detector descriptor and matcher\n",
    "        self.detector = cv2.FeatureDetector_create(DETECTOR)\n",
    "\n",
    "        if \"grid\" in DETECTOR.lower():\n",
    "            grid_adapted = cv2.GridAdaptedFeatureDetector\n",
    "            self.detector = grid_adapted(self.detector, MATCH_QTY/3, 6, 6)\n",
    "            self.descriptor = cv2.DescriptorExtractor_create(DESCRIPTOR)\n",
    "\n",
    "        if MATCHER == MATCHER_TYPES[1]: # BruteForce\n",
    "            self.matcher = cv2.BFMatcher(MTCH_METHOD, crossCheck=False)\n",
    "\n",
    "        # Data attributes storing time tracking\n",
    "        self.t0 = None\n",
    "        self.t_prev = None\n",
    "        self.t_curr = None\n",
    "        self.dt = None\n",
    "\n",
    "        # previous points and tracked points (list of dequoues)\n",
    "        self.points = []\n",
    "        self.tracks = []\n",
    "\n",
    "        # Data attributes storing time tracking\n",
    "        self.t0 = None\n",
    "        self.t_prev = None\n",
    "        self.t_curr = None\n",
    "        self.dt = None\n",
    "\n",
    "        # previous points and tracked points (list of dequoues)\n",
    "        self.points = []\n",
    "        self.tracks = []\n",
    "    \n",
    "    \n",
    "    def compare(self, image):\n",
    "        \"\"\"\n",
    "        Action: Compares two images using selected method\n",
    "        Input: Image instance\n",
    "        Output: (Current keypoints, Previous Keypoints)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Advance time tracking with new image time\n",
    "        frame_time = image.get_time()\n",
    "        self.__track_time(frame_time)\n",
    "        \n",
    "        # Advance state machine image storage\n",
    "        self.frame_prev = self.frame_curr\n",
    "        self.frame_curr = image\n",
    "        \n",
    "        # Calculate optical flow for keypoints. Determine new from old position\n",
    "        new_points, old_points = self.__optical_flow(tracking=True)\n",
    "        \n",
    "        return new_points, old_points\n",
    "    \n",
    "    def get_dt(self):\n",
    "        \"\"\"\n",
    "        Action: Returns delta t between two frames\n",
    "        Input: None\n",
    "        Output: time difference between current and previous frame\n",
    "        \"\"\"\n",
    "        return self.dt\n",
    "    \n",
    "    def __detector_descriptor(self, image, mask=None):\n",
    "        \"\"\"\n",
    "        Action: Add key points and key point descriptors to Image instance\n",
    "        Input: Image object being tested\n",
    "        optional mask of areas to test (same shape as frame)\n",
    "        Output: Updated image object with keypoints and descriptors\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get grey scale frame and create mask of ones if nothing passed in.\n",
    "        frame_grey = image.get_grey()\n",
    "        mask = np.ones_like(frame_grey) if mask is None else mask\n",
    "        \n",
    "        # Detect keypoints and time function call\n",
    "        TIMER.set()\n",
    "        kp = self.detector.detect(frame_grey, mask=mask)\n",
    "        TIMER.delta(\"Detector\")\n",
    "        \n",
    "        # Create descriptors for keypoints and time function call\n",
    "        TIMER.set()\n",
    "        (kp, desc) = self.descriptor.compute(frame_grey, kp)\n",
    "        TIMER.delta(\"Descriptor\")\n",
    "        \n",
    "        # Lazily set keypoint and descriptors on Image object\n",
    "        image.set_details(kp, desc)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def __match(self):\n",
    "        \"\"\"\n",
    "        Action: Determine matches between two frames\n",
    "        Input: None\n",
    "        Output: List of match objects (each object has index of train and query\n",
    "        descriptors. Use to create list of matching features)\n",
    "        Note: Call this function after using \"__detector_descriptor\" method\n",
    "        \"\"\"\n",
    "        \n",
    "        # Access descriptors from current and previous frame.\n",
    "        # Need descriptors from both images to create matches.\n",
    "        desc_curr = self.frame_curr.get_desc()\n",
    "        desc_prev = self.frame_prev.get_desc()\n",
    "        \n",
    "        # If there are no descriptors associated with previous frame default\n",
    "        # is to return empty list (meaning no matches)\n",
    "        if desc_prev is None:\n",
    "            return []\n",
    "        \n",
    "        if len(desc_prev) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Try to perform knn (k-nearest-neighbors) match. Return two closest\n",
    "        # matches for keypoint filtering by Lowe ratio test\n",
    "        \n",
    "        TIMER.set()\n",
    "        knn_kwargs = {'queryDescriptors': desc_curr,\n",
    "                      'trainDescriptors': desc_prev,\n",
    "                       'k': 2}\n",
    "        try:\n",
    "            raw_matches = self.matcher.knnMatch(**knn_kwargs)\n",
    "        \n",
    "        except cv2.error:\n",
    "            raw_matches = []\n",
    "        \n",
    "        else:\n",
    "            TIMER.delta(\"Matching\")\n",
    "        \n",
    "        # Perform Lowe-ratio test to discard 90% of false matches and reject\n",
    "        # less than 5% of correct matches\n",
    "        \n",
    "        matches = self.__lowe_filtering(raw_matches)\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    def __optical_flow(self, tracking=False):\n",
    "        \"\"\"\n",
    "        Action: Perform point introductions if fewer keypoints than threshold\n",
    "        Track keypoints with Lucas-Kanade optical flow w/ filtering\n",
    "        Input: Optional flag for tracking: retain point correspondences for\n",
    "        MAX_TRACK_LENGTH number of frames\n",
    "        Output: returned_new_keypoints - current frame keypoints\n",
    "        returned_old_keypoints - previous frame keypoints\n",
    "        \"\"\"\n",
    "        \n",
    "        # global constants which have largest impact on processing time\n",
    "        global MAX_TRACK_LENGTH\n",
    "        global MATCH_QTY\n",
    "        global LK_BACKCALC_PIXEL_DEVIATION\n",
    "        \n",
    "        # Frame margin ignored to prevent index out of bounds from descriptors.\n",
    "        EDGE_MARGIN = 5\n",
    "        \n",
    "        # Default is to return empty list of points\n",
    "        returned_new_points = []\n",
    "        returned_old_points = []\n",
    "        \n",
    "        # if compare function has only been called once, no frame to compare to\n",
    "        if not self.frame_prev:\n",
    "            return returned_new_points, returned_old_points\n",
    "        \n",
    "        points = self.points\n",
    "        points_detected = False\n",
    "        \n",
    "        # If fewer points than threshold, introduce new keypoints\n",
    "        # NOTE: If entered, this section may modify points before performing\n",
    "        # Optical Flow calc under next conditional.\n",
    "        # NOTE: This section may be entered knowing points will not be matched\n",
    "        # and points NOT introduced as the descriptors from the current\n",
    "        # frame need to be readied for the following frame.\n",
    "        \n",
    "        if len(points) < MATCH_QTY:\n",
    "            # Perform detection, description and matching\n",
    "            self.frame_curr = self.__detector_descriptor(self.frame_curr)\n",
    "            matches = self.__match()\n",
    "            points_detected = True\n",
    "            \n",
    "            TIMER.set()\n",
    "            # The remainder of the point introduction block concatenates the\n",
    "            # newly discovered keypoints onto the previous set of points.\n",
    "            #\n",
    "            # *** This is actually slightly tricky ***\n",
    "            #\n",
    "            # Because the previous points are all defined relative to the train\n",
    "            # image we MUST use the keypoints from the train image rather than\n",
    "            # query image. (despite the query being newer). This way, when the\n",
    "            # LK-OptFlow algorithm is executed in the next block, there will be\n",
    "            # agreement between the carried over points which associate with\n",
    "            # the previous image. If this isn’t done and new points are used,\n",
    "            # the (x,y) positions which associate with the new frame will be\n",
    "            # applied to the previous frame and will almost certainly be\n",
    "            # different keypoints. This means the points will not be as robust\n",
    "            # as those we’ve just worked so hard to acquire.\n",
    "            \n",
    "            kp_prev = self.frame_prev.get_kp()\n",
    "            \n",
    "            # Perform indexing to create list of (x,y) coords for best matches\n",
    "            best_kp_prev = [kp_prev[m.trainIdx].pt for m in matches]\n",
    "            \n",
    "            # If tracking, create new deques for newly introduced points\n",
    "            if tracking:\n",
    "                for x, y in best_kp_prev:\n",
    "                    self.tracks.append(deque([(x, y)],maxlen=MAX_TRACK_LENGTH))\n",
    "            \n",
    "            # Optical flow algorithm executes next using \"points\" variable.\n",
    "            # Conditional makes sure optical flow gets newly introduced AND\n",
    "            # retained points both associated with the older frame.\n",
    "            # Also deals with edge cases when no points are introduced.\n",
    "            #\n",
    "            # If no old points or new points, points is an empty list\n",
    "            if len(points) == 0 and len(best_kp_prev) == 0:\n",
    "                points = []\n",
    "            # If no old points, just float and return newest points\n",
    "            elif len(points) == 0:\n",
    "                points = np.float32(best_kp_prev)\n",
    "            # If no new points are found, return previous points\n",
    "            elif len(best_kp_prev) == 0:\n",
    "                pass\n",
    "            # Otherwise, concatenate new and old points and return\n",
    "            else:\n",
    "                points = np.concatenate((points, np.float32(best_kp_prev)), 0)\n",
    "            \n",
    "            returned_new_points = points\n",
    "            TIMER.delta(\"Point Insertion\")\n",
    "            \n",
    "            # Perform optical flow calculations if there are keypoints\n",
    "            if len(points) > 0:\n",
    "                # Store references to frame size, current and previous grey scales\n",
    "                width, height = self.frame_curr.get_size()\n",
    "                grey_curr = self.frame_curr.get_grey()\n",
    "                grey_prev = self.frame_prev.get_grey()\n",
    "                \n",
    "                # Renamed to shorten most function calls\n",
    "                kp_0 = points\n",
    "                \n",
    "                # Perform optical flow calculations\n",
    "                TIMER.set()\n",
    "                lk_args = [grey_prev, grey_curr, kp_0, None]\n",
    "                kp_1, sts, flg = cv2.calcOpticalFlowPyrLK(*lk_args, **LK_PARAMS)\n",
    "                TIMER.delta(\"LK-Flow\")\n",
    "                \n",
    "                new_points = kp_1\n",
    "                old_points = kp_0\n",
    "                \n",
    "                # Filtering steps\n",
    "                # Knock off all keypoints within the edge margin. This stops index\n",
    "                # errors for descriptors outside or on frame margin. Also, this\n",
    "                # means for moving frame, points moving ’off’ the frame are dropped\n",
    "                # so new points may be introduced\n",
    "                \n",
    "                TIMER.set()\n",
    "                \n",
    "                x_good = (new_points[:, 0] > EDGE_MARGIN) & \\\n",
    "                (new_points[:, 0] < width - EDGE_MARGIN)\n",
    "                y_good = (new_points[:, 1] > EDGE_MARGIN) & \\\n",
    "                (new_points[:, 1] < height - EDGE_MARGIN)\n",
    "                \n",
    "                # Performing filtering by running LK-OptFlow ::backwards:: meaning,\n",
    "                # use the query image as the train image and train image as the\n",
    "                # query image. Only take points that match in both directions.\n",
    "                \n",
    "                lk_args = [grey_curr, grey_prev, kp_1, None]\n",
    "                kp_0r, sts, flg = cv2.calcOpticalFlowPyrLK(*lk_args, **LK_PARAMS)\n",
    "                pxl_delta = abs(kp_0-kp_0r).max(1)\n",
    "                \n",
    "                r_good = pxl_delta < LK_BACKCALC_PIXEL_DEVIATION\n",
    "                TIMER.delta(\"LK-Back-Filter\")\n",
    "                TIMER.set()\n",
    "                \n",
    "                # Create a mask which is True for points that are not in margins,\n",
    "                # and meet filtering requirements\n",
    "                good = r_good & x_good & y_good\n",
    "                \n",
    "                # Perform indexing to select high quality keypoints\n",
    "                indices = [i for (i, flag) in enumerate(good) if flag]\n",
    "                \n",
    "                filtered_new_points = new_points[indices, :]\n",
    "                filtered_old_points = old_points[indices, :]\n",
    "                \n",
    "                # drop tracks whose points have not persisted and update detected\n",
    "                if tracking:\n",
    "                    good_tracks = zip(good, self.tracks)\n",
    "                    new_tracks = [track for (flag, track) in good_tracks if flag]\n",
    "                    for point, new_track in zip(filtered_new_points, new_tracks):\n",
    "                        new_track.append(point)\n",
    "                self.tracks = new_tracks\n",
    "                \n",
    "                returned_new_points = filtered_new_points\n",
    "                returned_old_points = filtered_old_points\n",
    "                \n",
    "                TIMER.delta(\"Track-Updates\")\n",
    "                \n",
    "                # Code optimization\n",
    "                # Run detector and descriptor if filtering has caused keypoints to\n",
    "                # fall below threshold. This way, detection and description are\n",
    "                # performed preemptively so new points may immediately be\n",
    "                # introduced in the next iteration. Preferable than calling\n",
    "                # detection-description twice on next iteration as this causes\n",
    "                # calculation speeds to fall outside of 30 hz region\n",
    "                \n",
    "                if len(filtered_new_points) < MATCH_QTY and not points_detected:\n",
    "                    self.frame_curr = self.__detector_descriptor(self.frame_curr)\n",
    "                    \n",
    "            self.points = returned_new_points\n",
    "            # New points and old points should always match in length as they\n",
    "            # should correspond. Simple test that may show errors with changes\n",
    "            # to code.\n",
    "            assert len(returned_new_points) == len(returned_old_points)\n",
    "            \n",
    "            return returned_new_points, returned_old_points\n",
    "        \n",
    "        @staticmethod\n",
    "        def __lowe_filtering(raw_matches):\n",
    "            \n",
    "            # Performs the Lowe ratio test on a matched set of keypoints\n",
    "            TIMER.set()\n",
    "            \n",
    "            # Match tolerance ranges\n",
    "            low_tol = 0\n",
    "            hgh_tol = 10\n",
    "            accepted = 6 # and lower\n",
    "            \n",
    "            # buckets matches based on ratio\n",
    "            match_quality = [[] for _ in xrange(low_tol, hgh_tol+1)]\n",
    "            try:\n",
    "                for (m, n) in raw_matches:\n",
    "                try:\n",
    "                    bucket = int(10*m.distance/n.distance)\n",
    "                except ZeroDivisionError as err:\n",
    "                    # two matches on top of one another. Rare but acceptable\n",
    "                    bucket = 0\n",
    "                \n",
    "                if bucket < hgh_tol:\n",
    "                    match_quality[bucket].append(m)\n",
    "            except ValueError:\n",
    "                match_quality[hgh_tol].extend([match[0] for match in raw_matches])\n",
    "\n",
    "            matches = []\n",
    "            num_matches = 0\n",
    "            \n",
    "            # Select best matches based on their ratio distance\n",
    "            for i in xrange(accepted):\n",
    "                bckt_qty = len(match_quality[i])\n",
    "                if num_matches > MATCH_QTY:\n",
    "                    break\n",
    "                elif bckt_qty < MATCH_QTY - num_matches:\n",
    "                    matches.extend(match_quality[i])\n",
    "                    num_matches += bckt_qty\n",
    "                else:\n",
    "                    matches.extend(match_quality[i][:MATCH_QTY - num_matches])\n",
    "                    num_matches += MATCH_QTY - num_matches + 1\n",
    "            \n",
    "            TIMER.delta(\"Lowe-Filtering\")\n",
    "            return matches\n",
    "        \n",
    "        def __track_time(self, frame_time=None):\n",
    "            \"\"\"\n",
    "            Private method for tracking dt, current and previous time steps\n",
    "            relative to an initial starting point\n",
    "            \"\"\"\n",
    "            # Store initial time\n",
    "            if self.t0 is None:\n",
    "                self.t0 = frame_time\n",
    "                t_prev = self.t_curr\n",
    "                t_curr = frame_time\n",
    "            \n",
    "            # frames are assumed to be sequential in time. Error if dt is negative\n",
    "            if t_prev is not None:\n",
    "                assert t_curr - t_prev >= 0\n",
    "                self.dt = t_curr - t_prev\n",
    "            \n",
    "            self.t_curr = t_curr\n",
    "            self.t_prev = t_prev\n",
    "            \n",
    "        def create_flow_plot(self, kp_0=None, kp_1=None):\n",
    "        \"\"\"\n",
    "        Action: Creates visualization of keypoint motion between frames. To\n",
    "        include color coding of keypoints based on optical flow rate,\n",
    "        kp_0, and kp_1 must be provided: the location of the previous\n",
    "        and current keypoints.\n",
    "        Input: Optional - kp_0 - previous keypoint positions\n",
    "        kp_1 - current keypoint positions\n",
    "        Output: Image object built from current frame, keypoints and keypoint\n",
    "        tracks.\n",
    "        Class Requirements: Current color image object\n",
    "        Current images Keypoints\n",
    "        (Uses) Keypoint track history when available.\n",
    "        \"\"\"\n",
    "        \n",
    "        colr_curr = self.frame_curr.get_color()\n",
    "        frame_time = self.frame_curr.get_time()\n",
    "        TIMER.set()\n",
    "        \n",
    "        # If user has passed previous and current points in use rate of motion\n",
    "        # to \"color code\" tracks based on rate of motion\n",
    "        \n",
    "        if kp_0 not in [None, []] and kp_1 not in [None, []]:\n",
    "            dxy = kp_1 - kp_0\n",
    "            dist = np.linalg.norm(dxy, axis=1)\n",
    "            rgb_vals = scalarMap.to_rgba(dist, bytes=True)[:, 0:3].astype(int)   \n",
    "            rgb_vals = map(tuple, rgb_vals)\n",
    "        # Otherwise just set all the track colors equally. This is faster.\n",
    "        else:\n",
    "            def color_iter():\n",
    "                while True:\n",
    "                    yield (0, 255, 0)\n",
    "            rgb_vals = color_iter()\n",
    "            \n",
    "        # plot the current points using a circle of the appropriate color.\n",
    "        for pt, color in zip(self.points, rgb_vals):\n",
    "            x, y = pt\n",
    "            cv2.circle(colr_curr, (x, y), 2, color, 1)\n",
    "        \n",
    "        # if tracking is turned on, plot the tracks with appropriate color.\n",
    "        if self.tracks is not []:\n",
    "            for track, color in zip(self.tracks, rgb_vals):\n",
    "                cv2.polylines(colr_curr, [np.int32(track)], False, color)\n",
    "        \n",
    "        # Plot the number of points being tracked at the top of the image.\n",
    "        draw_str(colr_curr, (20, 20), ’track count: %d’ % len(self.points))\n",
    "        TIMER.delta(\"Plotting-Tracks\")\n",
    "        \n",
    "        # Return the new image for display.\n",
    "        colr_curr = Image(colr_curr, frame_time=frame_time)\n",
    "        return colr_curr\n",
    "    \n",
    "    def create_grid_plot(self, cell_mags, (num_rows, num_cols), max_norm=10.0):\n",
    "        \"\"\"\n",
    "        Action: Creates a bar plot in each cell of an m x n grid. Bar plot will\n",
    "        be x and y directions.\n",
    "        Input: cell_mags - list of x-y magnitude [(x1, y1),(x2,y2),...,(xn,yn)]\n",
    "        num_rows - number of rows bar plots will be applied to.\n",
    "        num_cols - number of cols bar plots will be applied to.\n",
    "        Class Requirements: Current color image object.\n",
    "        \"\"\"\n",
    "        \n",
    "        colr_curr = self.frame_curr.get_color()\n",
    "        frame_time = self.frame_curr.get_time()\n",
    "        if cell_mags == [] or cell_mags == None:\n",
    "            return self.frame_curr\n",
    "        \n",
    "        TIMER.set()\n",
    "        \n",
    "        width, height = self.frame_curr.get_size()\n",
    "        \n",
    "        cols = np.linspace(0, width, num_cols+1)\n",
    "        rows = np.linspace(0, height, num_rows+1)\n",
    "        \n",
    "        col_width = (cols[1]-cols[0])\n",
    "        row_width = (rows[1]-rows[0])\n",
    "        \n",
    "        col_centers = cols[1:] - col_width/2.0\n",
    "        row_centers = rows[1:] - row_width/2.0\n",
    "        \n",
    "        rect_half_width = 5\n",
    "        rect_max_length = width/num_cols\n",
    "        rect_max_height = height/num_rows\n",
    "        \n",
    "        for i, mags in enumerate(cell_mags):\n",
    "            col_indx = i % num_cols\n",
    "            row_indx = i / num_cols\n",
    "            cent_xy = (col_centers[col_indx], row_centers[row_indx])\n",
    "            \n",
    "            # x-directional rectangle\n",
    "            if np.isnan(mags[0]):\n",
    "                pass\n",
    "            else:\n",
    "                x1 = int(cent_xy[0])\n",
    "                y1 = int(cent_xy[1] - rect_half_width)\n",
    "                x2 = int(cent_xy[0] + mags[0]/max_norm*rect_max_length)\n",
    "                y2 = int(cent_xy[1] + rect_half_width)\n",
    "                cv2.rectangle(colr_curr, (x1, y1), (x2, y2), (0, 255, 0), -1)\n",
    "            \n",
    "            # y-directional rectangle\n",
    "            if np.isnan(mags[1]):\n",
    "                pass\n",
    "            else:\n",
    "                x1 = int(cent_xy[0] - rect_half_width)\n",
    "                y1 = int(cent_xy[1])\n",
    "                x2 = int(cent_xy[0] + rect_half_width)\n",
    "                y2 = int(cent_xy[1] + mags[1]/max_norm*rect_max_height)\n",
    "                cv2.rectangle(colr_curr, (x1, y1), (x2, y2), (0, 0, 255), -1)\n",
    "            \n",
    "        TIMER.delta(\"Plotting-Grid\")\n",
    "        return Image(colr_curr, frame_time=frame_time)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:car]",
   "language": "python",
   "name": "conda-env-car-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
